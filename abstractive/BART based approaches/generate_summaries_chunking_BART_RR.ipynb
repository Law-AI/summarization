{"cells":[{"cell_type":"markdown","id":"646853af","metadata":{"id":"646853af"},"source":["### Script to generate summaries using chunking based BART RR method\n","\n","Assign the dataset and output_path variable according to requirements.  "]},{"cell_type":"code","execution_count":null,"id":"3fd5817d","metadata":{"id":"3fd5817d"},"outputs":[],"source":["dataset = \"N2\" # Options: IN - IN-Abs, UK-UK-Abs, N2-IN-Ext \n","output_path = \"./output/\""]},{"cell_type":"code","execution_count":null,"id":"ad7a3c99","metadata":{"id":"ad7a3c99"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '../')\n","import transformers\n","import pandas as pd\n","import numpy as np\n","import glob\n","import nltk\n","import torch\n","import math\n","import random\n","import re\n","import argparse\n","import os\n","from utilities import *"]},{"cell_type":"code","execution_count":null,"id":"383b92c3","metadata":{"id":"383b92c3"},"outputs":[],"source":["#Reading the test documents\n","names, data_source = get_summary_data_rhet_test(dataset)\n","print(len(names))\n","print(len(data_source))\n","dict_names = get_req_len_dict(dataset, \"test\") "]},{"cell_type":"code","execution_count":null,"id":"ea21bb4b","metadata":{"id":"ea21bb4b"},"outputs":[],"source":["# Loading Model and tokenizer\n","from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BartConfig\n","\n","\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large', add_prefix_space=True)\n","\n","model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n","\n","new_tokens = ['<F>', '<RLC>', '<A>', '<S>', '<P>', '<R>', '<RPC>']\n","# tokenizer.add_special_tokens(new_tokens)\n","\n","special_tokens_dict = {'additional_special_tokens': new_tokens}\n","num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","id":"b3aa4218","metadata":{"id":"b3aa4218"},"source":["#### Add the path to fine tuned model"]},{"cell_type":"code","execution_count":null,"id":"865f3020","metadata":{"id":"865f3020"},"outputs":[],"source":["bart_model = LitModel(learning_rate = 2e-5, tokenizer = tokenizer, model = model)\n","# bart_model = LitModel.load_from_checkpoint(\"path to model\",learning_rate = 2e-5, tokenizer = tokenizer, model = model).to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"id":"76586532","metadata":{"id":"76586532"},"outputs":[],"source":["def nest_sentencesV3(doc, chunk_length):\n","    '''\n","    function to first segment the document using rhetorical roles and then chunk if required\n","    input:  doc_sents           - Input document sentence\n","            chunk_length        - chunk length\n","    output: list of chunks\n","    '''\n","    doc_sents, _, dict_sents_labels = get_doc_sens_and_labels(doc)\n","    s = list(set(dict_sents_labels.values()))\n","#     print(s)\n","    all_chunks = []\n","    \n","    for label in s:\n","        doc_sents_withlabels = []\n","        for sent in doc_sents:\n","            if sent == '':continue\n","            if dict_sents_labels[sent] == label:\n","                doc_sents_withlabels.append(sent)\n","        chunks = nest_sentencesMV2(doc_sents_withlabels, chunk_length)\n","        \n","        edited_chunks = []\n","        for chunk in chunks:\n","            edited_chunks.append([\"<\" + label + \">\"] + chunk)\n","        #modified\n","        \n","        all_chunks = all_chunks + ['. '.join(i) for i in edited_chunks]\n","\n","    return all_chunks    \n"]},{"cell_type":"code","execution_count":null,"id":"39eae844","metadata":{"id":"39eae844"},"outputs":[],"source":["def generate_summary_gpu(nested_sentences,p):\n","    '''\n","    Function to generate summaries from the list containing chunks of the document\n","    input:  nested_sentences - chunks\n","            p - Number of words in summaries per word in the document\n","    output: document summary\n","    '''\n","    device = 'cuda'\n","    summaries = []\n","    for nested in nested_sentences:\n","        l = int(p * len(nested.split(\" \")))\n","        input_tokenized = tokenizer.encode(nested, truncation=True, return_tensors='pt')\n","        input_tokenized = input_tokenized.to(device)\n","        summary_ids = bart_model.model.to(device).generate(input_tokenized,\n","                                          length_penalty=0.05,\n","                                          min_length=l-5,\n","                                          max_length=l+5)\n","\n","        output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n","        summaries.append(output)\n","    summaries = [sentence for sublist in summaries for sentence in sublist]\n","    return summaries"]},{"cell_type":"code","execution_count":null,"id":"85033199","metadata":{"id":"85033199"},"outputs":[],"source":["import os\n","if not os.path.exists(output_path):\n","    os.makedirs(output_path)"]},{"cell_type":"code","execution_count":null,"id":"2bdf4344","metadata":{"id":"2bdf4344"},"outputs":[],"source":["# main loop to generate and save summaries of each document in the test dataset\n","output = []\n","for i in range(len(data_source)):\n","    name = names[i]\n","    doc = data_source[i]\n","    wc = doc.split(\" \")\n","    input_len = len(wc)\n","    req_len = dict_names[name]\n","    print(str(i) + \": \" + name +  \" - \" + str(input_len) + \" : \" + str(req_len), end = \", \")\n","    \n","    nested = nest_sentencesV3(doc,1024)\n","    p = float(req_len/input_len)\n","    print(p)\n","    abs_summ = generate_summary_gpu(nested,p)\n","    abs_summ = \" \".join(abs_summ)\n","    print(len((abs_summ.split(\" \"))))\n","    \n","    if len(abs_summ.split(\" \")) > req_len:\n","        abs_summ = abs_summ.split(\" \")\n","        abs_summ = abs_summ[:req_len]\n","        abs_summ = \" \".join(abs_summ)\n","#     print(abs_summ)\n","#     break\n","    print(len((abs_summ.split(\" \"))))\n","    path = output_path + name\n","    file = open(path,'w')\n","    file.write(abs_summ)\n","    file.close()\n","    \n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"62a88525","metadata":{"id":"62a88525"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"generate_summaries_chunking_BART_RR.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}